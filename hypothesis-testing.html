<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Hypothesis Testing | Statistics</title>
  <meta name="description" content="9 Hypothesis Testing | Statistics" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Hypothesis Testing | Statistics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dyrehaugen/rsts" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Hypothesis Testing | Statistics" />
  
  
  

<meta name="author" content="Dyrehaugen Web Notebook" />


<meta name="date" content="2022-06-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="causation.html"/>
<link rel="next" href="p-test.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Statistics</a></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#intuition-for-probability"><i class="fa fa-check"></i><b>2.1</b> Intuition for Probability</a></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#pandemic-risk-management"><i class="fa fa-check"></i><b>2.2</b> Pandemic Risk Management</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="big-data-paradox.html"><a href="big-data-paradox.html"><i class="fa fa-check"></i><b>3</b> Big Data Paradox</a>
<ul>
<li class="chapter" data-level="3.1" data-path="big-data-paradox.html"><a href="big-data-paradox.html#quarantine-fatigue-thins-fat-tailed-impacts"><i class="fa fa-check"></i><b>3.1</b> Quarantine fatigue thins fat-tailed impacts</a></li>
<li class="chapter" data-level="3.2" data-path="big-data-paradox.html"><a href="big-data-paradox.html#herd-immunity-impossible-with-new-mutants"><i class="fa fa-check"></i><b>3.2</b> Herd Immunity impossible with new Mutants</a></li>
<li class="chapter" data-level="3.3" data-path="big-data-paradox.html"><a href="big-data-paradox.html#danish-mask-study"><i class="fa fa-check"></i><b>3.3</b> Danish Mask Study</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="fat-tails.html"><a href="fat-tails.html"><i class="fa fa-check"></i><b>4</b> Fat Tails</a>
<ul>
<li class="chapter" data-level="4.1" data-path="fat-tails.html"><a href="fat-tails.html#extremes"><i class="fa fa-check"></i><b>4.1</b> Extremes</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="fat-tails.html"><a href="fat-tails.html#catastrophe-principle"><i class="fa fa-check"></i><b>4.1.1</b> Catastrophe Principle</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="fat-tails.html"><a href="fat-tails.html#statistical-consequences-of-fat-tails"><i class="fa fa-check"></i><b>4.2</b> Statistical Consequences of Fat Tails</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="fat-tails.html"><a href="fat-tails.html#power-law-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Power Law Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="fat-tails.html"><a href="fat-tails.html#lindy-effect"><i class="fa fa-check"></i><b>4.3</b> Lindy Effect</a></li>
<li class="chapter" data-level="4.4" data-path="fat-tails.html"><a href="fat-tails.html#superspreaders"><i class="fa fa-check"></i><b>4.4</b> Superspreaders</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="vaccine.html"><a href="vaccine.html"><i class="fa fa-check"></i><b>5</b> Vaccine</a></li>
<li class="chapter" data-level="6" data-path="inequality.html"><a href="inequality.html"><i class="fa fa-check"></i><b>6</b> Inequality</a></li>
<li class="chapter" data-level="7" data-path="autocorrelation.html"><a href="autocorrelation.html"><i class="fa fa-check"></i><b>7</b> Autocorrelation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="autocorrelation.html"><a href="autocorrelation.html#dunning-kruger-is-autocorrelation"><i class="fa fa-check"></i><b>7.1</b> Dunning-Kruger is Autocorrelation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causation.html"><a href="causation.html"><i class="fa fa-check"></i><b>8</b> Causation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="causation.html"><a href="causation.html#liang-causality"><i class="fa fa-check"></i><b>8.1</b> Liang Causality</a></li>
<li class="chapter" data-level="8.2" data-path="causation.html"><a href="causation.html#causation-in-chaotic-dynamic-systems"><i class="fa fa-check"></i><b>8.2</b> Causation in Chaotic Dynamic Systems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>9</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#connecting-to-theory"><i class="fa fa-check"></i><b>9.1</b> Connecting to Theory</a></li>
<li class="chapter" data-level="9.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#glmm"><i class="fa fa-check"></i><b>9.2</b> GLMM</a></li>
<li class="chapter" data-level="9.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#logit"><i class="fa fa-check"></i><b>9.3</b> Logit</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#odds-ratio"><i class="fa fa-check"></i><b>9.3.1</b> Odd’s Ratio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="p-test.html"><a href="p-test.html"><i class="fa fa-check"></i><b>10</b> P test</a>
<ul>
<li class="chapter" data-level="10.1" data-path="p-test.html"><a href="p-test.html#p-value-hacking"><i class="fa fa-check"></i><b>10.1</b> P-Value Hacking</a></li>
<li class="chapter" data-level="10.2" data-path="p-test.html"><a href="p-test.html#bootstrapping-instead-of-p-values"><i class="fa fa-check"></i><b>10.2</b> Bootstrapping instead of p-values</a></li>
<li class="chapter" data-level="10.3" data-path="p-test.html"><a href="p-test.html#probit"><i class="fa fa-check"></i><b>10.3</b> Probit</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="spurious-correlation.html"><a href="spurious-correlation.html"><i class="fa fa-check"></i><b>11</b> Spurious Correlation</a>
<ul>
<li class="chapter" data-level="11.1" data-path="spurious-correlation.html"><a href="spurious-correlation.html#trending-variables"><i class="fa fa-check"></i><b>11.1</b> Trending Variables</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="stationarity.html"><a href="stationarity.html"><i class="fa fa-check"></i><b>12</b> Stationarity</a>
<ul>
<li class="chapter" data-level="12.1" data-path="stationarity.html"><a href="stationarity.html#record-events"><i class="fa fa-check"></i><b>12.1</b> Record Events</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="power-laws.html"><a href="power-laws.html"><i class="fa fa-check"></i><b>13</b> Power laws</a>
<ul>
<li class="chapter" data-level="13.1" data-path="power-laws.html"><a href="power-laws.html#generative-models"><i class="fa fa-check"></i><b>13.1</b> Generative Models</a></li>
<li class="chapter" data-level="13.2" data-path="power-laws.html"><a href="power-laws.html#income-distribution-power-law"><i class="fa fa-check"></i><b>13.2</b> Income Distribution Power Law</a></li>
<li class="chapter" data-level="13.3" data-path="power-laws.html"><a href="power-laws.html#timescaling-rainfall"><i class="fa fa-check"></i><b>13.3</b> Timescaling Rainfall</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="syntetic-control.html"><a href="syntetic-control.html"><i class="fa fa-check"></i><b>14</b> Syntetic Control</a></li>
<li class="chapter" data-level="15" data-path="econometrics.html"><a href="econometrics.html"><i class="fa fa-check"></i><b>15</b> Econometrics</a></li>
<li class="part"><span><b>I Appendices</b></span></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i><b>A</b> About</a></li>
<li class="chapter" data-level="B" data-path="links.html"><a href="links.html"><i class="fa fa-check"></i><b>B</b> Links</a></li>
<li class="chapter" data-level="C" data-path="news.html"><a href="news.html"><i class="fa fa-check"></i><b>C</b> NEWS</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dyrehaugen/rsts" target="blank">On Github</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-testing" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Hypothesis Testing</h1>
<p>Some text here …</p>
<p><a href="https://cran.r-project.org/web/packages/distributions3/vignettes/intro-to-hypothesis-testing.html">Cran: Intro Hyp in R</a></p>
<p><a href="https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture07/lecture07-94842.html">Chouldechova: Hyp in R</a></p>
<div id="connecting-to-theory" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Connecting to Theory</h2>
<p><em>Memo</em></p>
<p>In order to bound the probability of Type 2 errors below a small value
we may have to accept a high probability of making a Type 1 error.</p>
<p><strong>Scheel</strong></p>
<p><em>Abstract</em></p>
<p>For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance
tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis,
reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary
and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they
may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established
a sound “derivation chain” between test and theory is counterproductive. Instead, various nonconfirmatory research
activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses,
researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships
between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary
assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we
discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology’s reform
movement and help us to develop strong, testable theories, as Paul Meehl urged.</p>
<p><em>Memo Scheel</em></p>
<p>Excessive leniency
in study design, data collection, and analysis led psy-
chological scientists to be overconfident about many
hypotheses that turned out to be false. In response, psy-
chological science as a field tightened the screws on the
machinery of confirmatory testing: Predictions should be
more specific, designs more powerful, and statistical tests
more stringent, leaving less room for error and misrepre-
sentation. Confirmatory testing will be taught as a highly
formalized protocol with clear rules, and the student will
learn to strictly separate it from the “exploratory” part of
the research process. Has learned how
to operate the hypothesis-testing machinery but not
how to feed it with meaningful input.</p>
<p>When setting up a hypothesis test, the researcher has to specify
how their independent and dependent variables will
be operationalized, how many participants they will
collect, which exclusion criteria they will apply, which
statistical method they will use, how to decide whether
the hypothesis was corroborated or falsified, and so on.
But deciding between these myriad options often feels
like guesswork.</p>
<p>A lack of knowledge about the elements that link their test
back to the theory from which their hypothesis was
derived. By using arbitrary defaults and heuristics to
bridge these gaps, the researcher cannot be sure how
their test result informs the theory.</p>
<p><a href="https://journals.sagepub.com/doi/full/10.1177/1745691620966795">Scheel(2020) Less Hypothesis Testing</a>
<a href="pdf/Scheel_2020_Less_Hypothesis_Testing.pdf">(pdf)</a></p>

</div>
<div id="glmm" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> GLMM</h2>
<p>The advent of generalized linear models has allowed us to build regression-type models of data when the distribution of the response variable is non-normal–for example, when your DV is binary. (If you would like to know a little more about GLiMs, I wrote a fairly extensive answer here, which may be useful although the context differs.) However, a GLiM, e.g. a logistic regression model, assumes that your data are independent. For instance, imagine a study that looks at whether a child has developed asthma. Each child contributes one data point to the study–they either have asthma or they don’t. Sometimes data are not independent, though. Consider another study that looks at whether a child has a cold at various points during the school year. In this case, each child contributes many data points. At one time a child might have a cold, later they might not, and still later they might have another cold. These data are not independent because they came from the same child. In order to appropriately analyze these data, we need to somehow take this non-independence into account. There are two ways: One way is to use the generalized estimating equations (which you don’t mention, so we’ll skip). The other way is to use a generalized linear mixed model. GLiMMs can account for the non-independence by adding random effects (as <span class="citation">(<span class="citeproc-not-found" data-reference-id="MichaelChernick"><strong>???</strong></span>)</span> notes). Thus, the answer is that your second option is for non-normal repeated measures (or otherwise non-independent) data. (I should mention, in keeping with <span class="citation">(<span class="citeproc-not-found" data-reference-id="Macro"><strong>???</strong></span>)</span>’s comment, that general-ized linear mixed models include linear models as a special case and thus can be used with normally distributed data. However, in typical usage the term connotes non-normal data.)</p>
<p>Update: (The OP has asked about GEE as well, so I will write a little about how all three relate to each other.)</p>
<p>Here’s a basic overview:</p>
<ul>
<li>a typical GLiM (I’ll use logistic regression as the prototypical case) lets you model an independent binary response as a function of covariates</li>
<li>a GLMM lets you model a non-independent (or clustered) binary response conditional on the attributes of each individual cluster as a function of covariates</li>
<li>the GEE lets you model the population mean response of non-independent binary data as a function of covariates</li>
</ul>
<p>Since you have multiple trials per participant, your data are not independent; as you correctly note, “[t]rials within one participant are likely to be more similar than as compared to the whole group”. Therefore, you should use either a GLMM or the GEE.</p>
<p>The issue, then, is how to choose whether GLMM or GEE would be more appropriate for your situation. The answer to this question depends on the subject of your research–specifically, the target of the inferences you hope to make. As I stated above, with a GLMM, the betas are telling you about the effect of a one unit change in your covariates on a particular participant, given their individual characteristics. On the other hand with the GEE, the betas are telling you about the effect of a one unit change in your covariates on the average of the responses of the entire population in question. This is a difficult distinction to grasp, especially because there is no such distinction with linear models (in which case the two are the same thing).</p>
<p>One way to try to wrap your head around this is to imagine averaging over your population on both sides of the equals sign in your model. For example, this might be a model:</p>
<p><span class="math display">\[logit(p_i)=β_0+β_1X_1+b_i\]</span></p>
<p>where:</p>
<p><span class="math inline">\(logit(p)=ln(\frac{p}{1−p})\)</span>, &amp; <span class="math inline">\(b∼N(0,σ_{b}^2)\)</span></p>
<p>There is a parameter that governs the response distribution (pp, the probability, with binary data) on the left side for each participant. On the right hand side, there are coefficients for the effect of the covariate[s] and the baseline level when the covariate[s] equals 0. The first thing to notice is that the actual intercept for any specific individual is not β0_0, but rather (β0+bi)(_0+b_i). But so what? If we are assuming that the bib_i’s (the random effect) are normally distributed with a mean of 0 (as we’ve done), certainly we can average over these without difficulty (it would just be β0_0). Moreover, in this case we don’t have a corresponding random effect for the slopes and thus their average is just β1_1. So the average of the intercepts plus the average of the slopes must be equal to the logit transformation of the average of the pip_i’s on the left, mustn’t it? Unfortunately, no. The problem is that in between those two is the logit, which is a non-linear transformation. (If the transformation were linear, they would be equivalent, which is why this problem doesn’t occur for linear models.) The following plot makes this clear:</p>
<p><img src="fig/glmm_logit_StackOverflow.png" /></p>
<p>Imagine that this plot represents the underlying data generating process for the probability that a small class of students will be able to pass a test on some subject with a given number of hours of instruction on that topic. Each of the grey curves represents the probability of passing the test with varying amounts of instruction for one of the students. The bold curve is the average over the whole class. In this case, the effect of an additional hour of teaching conditional on the student’s attributes is β1_1–the same for each student (that is, there is not a random slope). Note, though, that the students baseline ability differs amongst them–probably due to differences in things like IQ (that is, there is a random intercept). The average probability for the class as a whole, however, follows a different profile than the students. The strikingly counter-intuitive result is this: an additional hour of instruction can have a sizable effect on the probability of each student passing the test, but have relatively little effect on the probable total proportion of students who pass. This is because some students might already have had a large chance of passing while others might still have little chance.</p>
<p>The question of whether you should use a GLMM or the GEE is the question of which of these functions you want to estimate. If you wanted to know about the probability of a given student passing (if, say, you were the student, or the student’s parent), you want to use a GLMM. On the other hand, if you want to know about the effect on the population (if, for example, you were the teacher, or the principal), you would want to use the GEE.</p>
<p><a href="https://stats.stackexchange.com/questions/32419/difference-between-generalized-linear-models-generalized-linear-mixed-models">StackOverFlow</a></p>
<p>What are the best methods for checking a generalized linear mixed model (GLMM) for proper fit?
Unfortunately, it isn’t as straightforward as it is for a general linear model.
n linear models the requirements are easy to outline: linear in the parameters, normally distributed and independent residuals, and homogeneity of variance (that is, similar variance at all values of all predictors).</p>
<p>For linear models, there are well-described and well-implemented methods for checking each of these, both visual/descriptive methods and statistical tests.</p>
<p>It is not nearly as easy for GLMMs.</p>
<p><strong>Assumption: Random effects come from a normal distribution</strong></p>
<p>Let’s start with one of the more familiar elements of GLMMs, which is related to the random effects. There is an assumption that random effects—both intercepts and slopes—are normally distributed.</p>
<p>These are relatively easy to export to a data set in most statistical software (including SAS and R). Personally, I much prefer visual methods of checking for normal distributions, and typically go right to making histograms or normal probability plots (Q-Q plots) of each of the random effects.</p>
<p>If the histograms look roughly bell-shaped and symmetric, or the Q-Q plots generally fall close to a diagonal line, I usually consider this to be good enough.</p>
<p>If the random effects are not reasonably normally distributed, however, there are not simple remedies. In a general linear model outcomes can be transformed. In GLMMs they cannot.</p>
<p>Research is currently being conducted on the consequences of mis-specifying the distribution of random effects in GLMMs. (Outliers, of course, can be handled the same way as in generalized linear models—except that an entire random subject, as opposed to a single observation, may be examined.)</p>
<p><strong>Assumption: The chosen link function is appropriate</strong></p>
<p>Additional assumptions of GLMMs are more related to the generalized linear model side. One of these is the relationship of the numeric predictors to the parameter of interest, which is determined by the link function.</p>
<p>For both generalized linear models and GLMMs, it is important to understand that the most typical link functions (e.g., the logit for binomial data, the log for Poisson data) are not guaranteed to be a good representation of the relationship of the predictors with the outcomes.</p>
<p>Checking this assumption can become quite complicated as models become more crowded with fixed and random effects.</p>
<p>One relatively simple (though not perfect) way to approach this is to compare the predicted values to the actual outcomes.</p>
<p>With most GLMMs, it is best to compare averages of outcomes to predicted values. For example, with binomial models, one could take all of the values with predicted values near 0.5, 0.15, 0.25, etc., and average the actual outcomes (the 0s and 1s). You can then plot these average values against the predicted values.</p>
<p>If the general form of the model is correct, the differences between the predicted values and the averaged actual values will be small. (Of course how small depends on the number of observations and variance function).</p>
<p>No “patterns” in these differences should be obvious.</p>
<p>This is similar to the idea of the Hosmer-Lemeshow test for logistic regression models. If you suspect that the form of the link function is not correct, there are remedies. Possibilites include changing the link function, transforming numeric predictors, or (if necessary) categorizing continuous predictors.</p>
<p><strong>Assumption: Appropriate estimation of variance</strong></p>
<p>Finally, it is important to check the variability of the outcomes. This is also not as easy as it is for linear models, since the variance is not constant and is a function of the parameter being estimated.</p>
<p>Fortunately, this is one of the easier assumptions to check. One of the fit statistics your statistical software produces is a generalized chi-square that compares the magnitude of the model residuals to the theoretical variance.</p>
<p>The chi-square divided by its degrees of freedom should be approximately 1. If this statistic is too large, then the variance is “overdispersed” (larger than it should be). Alternatively, if the statistic is too small, the variance is “underdispersed.”</p>
<p>While the best way to approach this varies by distribution, there are options to adjust models for overdispersion that result in more conservative p-values.</p>
<p><a href="https://www.theanalysisfactor.com/regression-diagnostics-glmm/">TheAnalysisFactor</a></p>

</div>
<div id="logit" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Logit</h2>
<p>Possible Analysis methods:</p>
<p>Below is a list of some analysis methods you may have encountered. Some of the methods listed are quite reasonable while others have either fallen out of favor or have limitations.</p>
<ul>
<li>Logistic regression, the focus of this page</li>
<li>Probit regression. Probit analysis will produce results similar logistic regression. The choice of probit versus logit depends largely on individual preferences.</li>
<li>OLS regression. When used with a binary response variable, this model is known as a linear probability model and can be used as a way to describe conditional probabilities. However, the errors (i.e., residuals) from the linear probability model violate the homoskedasticity and normality of errors assumptions of OLS regression, resulting in invalid standard errors and hypothesis tests. For a more thorough discussion of these and other problems with the linear probability model.</li>
<li>Two-group discriminant function analysis. A multivariate method for dichotomous outcome variables.</li>
<li>Hotelling’s T2. The 0/1 outcome is turned into the grouping variable, and the former predictors are turned into outcome variables. This will produce an overall test of significance but will not give individual coefficients for each variable, and it is unclear the extent to which each “predictor” is adjusted for the impact of the other “predictors.”</li>
</ul>
<p><a href="https://stats.idre.ucla.edu/r/dae/logit-regression/">ucla</a></p>
<div id="odds-ratio" class="section level3" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Odd’s Ratio</h3>
<p>If you want to interpret the estimated effects as relative odds ratios,
just do <code>exp(coef(x))</code> (gives you <span class="math inline">\(e^β\)</span>,
the multiplicative change in the odds ratio for <span class="math inline">\(y=1\)</span>
if the covariate associated with <span class="math inline">\(β\)</span> increases by 1).</p>
<p>For profile likelihood intervals for this quantity, you can do</p>
<pre><code>require(MASS)
exp(cbind(coef(x), confint(x)))  </code></pre>
<p>To get the odds ratio, we need the classification cross-table
of the original dichotomous DV and
the predicted classification according to some probability threshold that needs to be chosen first.</p>
<p><a href="https://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio">StackOverFlow</a></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="causation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="p-test.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dyrehaugen/rsts/edit/master/201-testing.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["rsts.pdf", "rsts.epub", "rsts.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
