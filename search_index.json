[["index.html", "Statistics 1 Statistics 1.1 Pandemic Risk Management 1.2 Quarantine fatigue thins fat-tailed impacts 1.3 Herd Immunity impossible with new Mutants 1.4 Danish Mask Study", " Statistics Dyrehaugen Web Notebook 2021-02-22 1 Statistics 1.1 Pandemic Risk Management Non-Ergodic Paranoia or Nothing Taleb and collegues have som very interesting methodological remarks in the early stages of the COVID-19 outbreak: Clearly, we are dealing with an extreme fat-tailed process owing to an increased connectivity, which increases the spreading in a nonlinear way. Fat tailed processes have special attributes, making conventional risk-management approaches inadequate The general (non-naive) precautionary principle delin- eates conditions where actions must be taken to reduce risk of ruin, and traditional cost-benefit analyses must not be used. These are ruin problems where, over time, exposure to tail events leads to a certain eventual extinction. While there is a very high probability for humanity surviving a single such event, over time, there is eventually zero probability of surviving repeated exposures to such events. While repeated risks can be taken by individuals with a limited life expectancy, ruin exposures must never be taken at the systemic and collective level. In technical terms, the precautionary principle applies when traditional statistical averages are invalid because risks are not ergodic. Historically based estimates of spreading rates for pandemics in general, and for the current one in particular, underestimate the rate of spread because of the rapid increases in transportation connectivity over recent years. This means that expectations of the extent of harm are under- estimates both because events are inherently fat tailed, and because the tail is becoming fatter as connectivity increases Estimates of the virus’s reproductive ratio \\(R\\_{0}\\) —the number of cases one case generates on average over the course of its infectious period in an otherwise uninfected population—are biased downwards. This property comes from fat-tailedness due to individual ‘superspreader’ events. Simply,\\(R\\_{0}\\) is estimated from an average which takes longer to converge as it is itself a fat-tailed variable. Norman/Bar-Yam/Taleb Note (pdf) 1.2 Quarantine fatigue thins fat-tailed impacts Abstract Conte: Fat-tailed damages across disease outbreaks limit the ability to learn and prepare for future outbreaks, as the central limit theorem slows down and fails to hold with infinite moments. We demonstrate the emergence and persistence of fat tails in contacts across the U.S. We then demonstrate an interaction between these contact rate distributions and community-specific disease dynamics to create fat-tailed distributions of COVID-19 impacts (proxied by weekly cumulative cases and deaths) during the exact time when attempts at suppression were most intense. Our stochastic SIR model implies the effective reproductive number also follows a fat-tailed stochastic process and leads to multiple waves of cases with unpredictable timing and magnitude instead of a single noisy wave of cases found in many compartmental models that introduce stochasticity via an additively-separable error term. Public health policies developed based on experiences during these months could be viewed as an overreaction if these impacts were mistakenly perceived as thin tailed, possibly contributing to reduced compliance, regulation, and the quarantine fatigue. While fat-tailed contact rates associated with superspreaders increase transmission and case numbers, they also suggest a potential benefit: targeted policy interventions are more effective than they would be with thin-tailed contacts. If policy makers have access to the necessary information and a mandate to act decisively, they might take advantage of fat-tailed contacts to prevent inaction that normalizes case and death counts that would seem extreme early in the outbreak. Our place-based estimates of contacts aid in these efforts by showing the dynamic nature of movement through communities as the outbreak progresses, which is quite costly to achieve in network models, forcing the assumption of static contact networks in many models. In extreme value theory, fat tails confound efforts to prepare for future extreme events like natural disasters and violent conflicts because experience does not provide reliable information about future tail draws. However, impacts of extreme events play out over time based on policy and behavioral responses to the event, which are themselves dynamically informed by past experiences. A general pattern of fat-tailed contact rate distributions across the U.S. suggests that fat tails in U.S. cases observed early in the outbreak are due to city- and county-specific contact networks and epidemiological dynamics. By unpacking the dynamics that lead to the impacts of extreme events, we show that 1) fat-tailed impacts can also confound efforts to control and manage impacts in the midst of extreme events and 2) thin tails in disease impacts are not necessarily desirable, if they indicate an inevitable catastrophe. Conte (2021) Quarantine fatigue thins fat-tailed coronavirus impacts (pdf) (pdf - SM) 1.3 Herd Immunity impossible with new Mutants Professor of vaccinology Shabir Madhi at the University of the Witwatersrand says protecting at-risk individuals against severe Covid is more important than herd immunity Leading vaccine scientists are calling for a rethink of the goals of vaccination programmes, saying that herd immunity through vaccination is unlikely to be possible because of the emergence of variants like that in South Africa. The comments came as the University of Oxford and AstraZeneca acknowledged that their vaccine will not protect people against mild to moderate Covid illness caused by the South African variant. Novavax and Janssen, which were trialled there in recent months and were found to have much reduced protection against the variant – at about 60%. Pfizer/BioNTech and Moderna have also said the variant affects the efficacy of their vaccines, although on the basis of lab studies only. These findings recalibrate thinking about how to approach the pandemic virus and shift the focus from the goal of herd immunity against transmission to the protection of all at-risk individuals in the population against severe disease. We probably need to switch to protecting the vulnerable, with the best vaccines we have which, although they don’t stop infection, they probably do stop you dying. Vaccine vs New Mutants (Guardian) 1.4 Danish Mask Study Every study needs its own statistical tools, adapted to the specific problem, which is why it is a good practice to require that statisticians come from mathematical probability rather than some software-cookbook school. When one uses canned software statistics adapted to regular medicine (say, cardiology), one is bound to make severe mistakes when it comes to epidemiological problems in the tails or ones where there is a measurement error. The authors of the study discussed below (The Danish Mask Study) both missed the effect of false positive noise on sample size and a central statistical signal from a divergence in PCR results. A correct computation of the odds ratio shows a massive risk reduction coming from masks. The article by Bundgaard et al., [“Effectiveness of Adding a Mask Recommendation to Other Public Health Measures to Prevent SARS-CoV-2 Infection in Danish Mask Wearers,” Annals of Internal Medicine (henceforth the “Danish Mask Study”)] relies on the standard methods of randomized control trials to establish the difference between the rate of infections of people wearing masks outside the house v.s. those who don’t (the control group), everything else maintained constant. The authors claimed that they calibrated their sample size to compute a p-value (alas) off a base rate of 2% infection in the general population. The result is a small difference in the rate of infection in favor of masks (2.1% vs 1.8%, or 42/2392 vs. 53/2470), deemed by the authors as not sufficient to warrant a conclusion about the effectiveness of masks. … Taleb’s Points: The Mask Group has 0/2392 PCR infections vs 5/2470 for the Control Group. Note that this is the only robust result and the authors did not test to see how nonrandom that can be. They missed on the strongest statistical signal. (One may also see 5 infections vs. 15 if, in addition, one accounts for clinically detected infections.) The rest, 42/2392 vs. 53/2470, are from antibody tests with a high error rate which need to be incorporated via propagation of uncertainty-style methods on the statistical significance of the results. Intuitively a false positive rate with an expected “true value” \\(p\\) is a random variable \\(\\rightarrow\\) Binomial Distribution with STD \\(\\sqrt{n p (1-p)}\\) False positives must be deducted in the computation of the odds ratio. The central problem is that both p and the incidence of infection are in the tails! As most infections happen at home, the study does not inform on masks in general –it uses wrong denominators for the computation of odds ratios (mixes conditional and unconditional risk). Worse, the study is not even applicable to derive information on masks vs. no masks outside the house since during most of the study (April 3 to May 20, 2020), “cafés and restaurants were closed “, conditions too specific and during which the infection rates are severely reduced –tells us nothing about changes in indoor activity. (The study ended June 2, 2020). A study is supposed to isolate a source of risk; such source must be general to periods outside the study (unlike cardiology with unconditional effects). The study does not take into account the fact that masks might protect others. Clearly this is not cardiology but an interactive system. Statistical signals compound. One needs to input the entire shebang, not simple individual tests to assess the joint probability of an effect. Comment from Tom Wenseleers For the 5 vs 0 PCR positive result the p value you calculate is flawed. The correct way to do it would e.g. be using a Firth logistic regression. Using R that would give you: library(brglm) summary(brglm(cbind(pcrpos, pcrneg) ~ treatment, family=binomial, data=data.frame(treatment=factor(c(“masks,””nomasks”)), pcrpos=c(0,5), pcrneg=c(2392,2470-5)))) 2-sided p=0.11. So that’s not significantly different. Alternatively, you might use a Fisher’s exact test, which would give you : fisher.test(cbind(c(0,2392),c(5,2470-5))): 2-sided p = 0.06. Again, not significantly different. A Firth logistic regression would be more appropriate though, since we have a clear outcome variable here and we don’t just want to test for an association in a 2×2 contingency table, as one would do using a Fisher’s exact test. For details see Firth, D. (1993). Bias reduction of maximum likelihood estimates. Biometrika 80, 27–38. A regular logistic regression doesn’t work here btw because of complete separation, https://en.wikipedia.org/wiki/Separation_(statistics) https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression. Going Bayesian would also be a solution, e.g. using the bayesglm() or brms package, or one could use an L1 or L2 norm or elastic net penalized binomial GLM model, e.g. using glmnet. But the p value you calculate above is definitely not correct. Sometimes it helps to not try to reinvent the wheel. The derivation of Fisher’s exact test you can find in most Statistics 101 courses, see e.g. https://mathworld.wolfram.com/FishersExactTest.html. For Firth’s penalized logistic regression, see https://medium.com/datadriveninvestor/firths-logistic-regression-classification-with-datasets-that-are-small-imbalanced-or-separated-49d7782a13f1 for a derivation. Or in Firth’s original article: https://www.jstor.org/stable/2336755?seq=1#metadata_info_tab_contents. Technically, the problem with the way you calculated your p value above is that you use a one-sample binomial test, and assume there is no sampling uncertainty on the p=5/2470. Which is obviously not correct. So you need a two-sample binomial test instead, which you could get via a logistic regression. But since you have complete separation you then can’t use a standard binomial GLM, and have to use e.g. a Firth penalized logistic regression instead. Anyway, the details are in the links above. You write “The probability of having 0 realizations in 2392 if the mean is \\(\\frac{5}{2470}\\) is 0.0078518, that is 1 in 127. We can reexpress it in p values, which would be &lt;.01.” This statement is obviously not correct then. And if you didn’t do p values – well, then your piece above is a little weak as a reply on how the authors should have done their hypothesis testing in a proper way, don’t you think? If the 0 vs 5 PCR positive result is not statistically significant I don’t see how you can make a sweeping statement like “The Mask Group has 0/2392 PCR infections vs 5/2470 for the Control Group. Note that this is the only robust result and the authors did not test to see how nonrandom that can be. They missed on the strongest statistical signal.” That “strong statistical signal” you mention turns out not be statistically significant at the p&lt;0.05 level if you do your stats properly… Taleb:You are conflating p values and statistical significance. Besides, I don’t do P values. https://arxiv.org/pdf/1603.07532.pdf you can also work with Bayes Factors if you like. Anything more formal than what you have above should do really… But just working with a PMF of a binomial distribution, and ignoring the sampling error on the 5/2470 control group is not OK. And if you’re worried about the accuracy of p values you could always still calculate 95% confidence limits on them, right? Also not really what people would typically consider p-hacking… Your title may a bit of a misnomer then. And as I mentioned: if one is worried about the accuracy of your p values &amp; stochasticity on its estimated value, you can always calculate p-value prediction intervals, https://royalsocietypublishing.org/doi/10.1098/rsbl.2019.0174. You are still ignoring the sampling uncertainty on the 0/2392. If you would like to go Monte Carlo you can use an exact-like logistic regression (https://www.jstatsoft.org/article/view/v021i03/v21i03.pdf). Using R, that gives me For the 0 vs 5 PCR positive result: library(elrm) set.seed(1) fit = elrm(pcrpos/n ~ treatment, ~ treatment, r=2, iter=400000, burnIn=1000, dataset=data.frame(treatment=factor(c(“masks,” “control”)), pcrpos=c(0, 5), n=c(2392, 2470)) ) fit\\(p.values # p value = 0.06, ie just about not significant at the 0.05 level fit\\)p.values.se # standard error on p value = 0.0003 # this is very close to the 2-sided Fisher exact test p value fisher.test(cbind(c(0,2392), c(5,2470-5))) # p value = 0.06 For the 0 vs 15 result: set.seed(1) fit = elrm(pcrpos/n ~ treatment, ~ treatment, r=2, iter=400000, burnIn=1000, dataset=data.frame(treatment=factor(c(“masks,” “control”)), pos=c(5, 15), n=c(2392, 2470)) ) fit\\(p.values # p value = 0.04 – this would be just about significant at the 0.05 level fit\\)p.values.se # standard error on p value = 0.0003 So some evidence for the opposite conclusions as what they have (especially for the 5 vs 15 result), but still not terribly strong. Details of method are in https://www.jstatsoft.org/article/view/v021i03/v21i03.pdf. I can see you don’t like canned statistics. And you could recode these kinds of methods quite easily in Mathematica if you like, see here for a Fisher’s exact test e.g.: https://mathematica.stackexchange.com/questions/41450/better-way-to-get-fisher-exact. But believe me – also Sir Ronald Fisher will have thought long and hard about these kinds of problems. And he would have seen in seconds that what you do above is simply not correct. Quite big consensus on that if I read the various comments here by different people… I was testing the hypothesis of there being no difference in infection rate between both groups and so was doing 2-sided tests. Some have argued that masks could actually make things worse if not used properly. So not doing a directional test would seem most objective to me. But if you insist, then yes, you could use 1-tailed p values… Then you would get 1-sided p values of 0.03 and 0.02 for the 0 vs 5 and 5 vs 15 sections of the data. Still deviates quite a bit from the p&lt;0.01 that you first had. In terms of double column joint distribution: then I think your code above should have e.g. 15/2470 and 5/2392 as your expectation of the Bernoulli distribution for vs 5 vs 15 comparison. But that would give problems for the 0/2392 outcome for the masks group in the 0 vs 5 comparison. As simulated Bernouilli trials with p=0 will be all zeros. Also, right now I don’t see where that 2400 was coming from in your code. I get that you are doing a one-sided two-sample binomial test here via a MC approach. That’s not the same than a Fisher exact test though. Andreas: Weird, the last part of my comment above apparently got chopped up somehow. Ignore the CI calculations as they got messed up, but are trivial. Trying again with the text that got lost, containing my main point: So the false positive-adjusted Odds Ratio is .71 [95% CI .41, 1.21], using the same model as the authors of the paper did. This can be compared to their reported OR = .82 [95% CI .54, 1.23]. Even with my quite conservative adjustment, the only robust finding claimed in the paper is not robust anymore – the estimated risk reduction is no longer significantly lower than 50%, according to the same standard logistic model used by the authors. Nor is it sig. larger than 0%. The CI did not really improve over the unadjusted one (maybe this was obvious a priori, but not to me). Either way I think .71 is a better estimate than the .82 that was reported in the paper, based on Nassim’s reasoning about the expected false positives. And .71 vs. .82 might well have crossed the line for a mask policy to be seriously considered, by some policymaker who rejected .82 as too close to 1. Sensitivity analysis of the FPR adjustment: 1% FPR (Nassim’s suggestion from the blog post) =&gt; OR = .66 [95% CI .36, 1.19] .5% FPR (lower estimate from the Bundgaard et al. paper, based on a previous study) =&gt; OR = .76 [95% CI .47, 1.22] Tom I do agree with all the shortcomings of this study in general though. It certainly was massively underpowered. Other comments: Bundgaard (2020) Effectiveness of Adding Mask Same in Annals (pdf) Taleb Review of Bundgaard Odd’s Ratio Explained (NIH) Composite Endpoints: Composite endpoints in clinical trials are composed of primary endpoints that contain two or more distinct component endpoints. The purported benefits include increased statistical efficiency, decrease in sample-size requirements, shorter trial duration, and decreased cost. However, the purported benefits must be diligently weighed against the inherent challenges in interpretation. Furthermore, the larger the gradient in importance, frequency, or results between the component endpoints, the less informative the composite endpoint becomes, thereby decreasing its utility for medical-decision making. [Composite Endpoints (NIH)] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6040910/) Separation (Wikipedia) "],["fat-tails.html", "2 Fat Tails 2.1 Extremes 2.2 Statistical Consequences of Fat Tails 2.3 Lindy Effect", " 2 Fat Tails Extremes Catastrophe Principle Statistical Consequences of Fat Tails Power Law Distributions Pareto Distribution Pandemic Risk Management Science uses statistics &amp;, as per Popper, doesn’t really “accept,” just fails to reject at some significance. It’s fundamentally disconfirmatory. Stat. “evidence” is inherently probabilistic and cannot be “degenerate” (i.e. provide certainties). (Taleb) 2.1 Extremes The field of Extreme Value Theory focuses on tail properties, not the mean or statistical inference. It is vastly more effective to focus on being insulated from the harm of random events than try to figure them out in the required details (the inferential errors under fat tails are huge). So it is more solid, much wiser, more ethical, and more effective to focus on detection heuristics and policies rather than fabricate statistical properties. 2.1.1 Catastrophe Principle Memo Taleb (DarwinCollege): Where a Pareto distribution prevails (among many), and randomly select two people with combined wealth of £36 million. The most likely combination is not £18 million and £18 million. It is approximately £35,999,000 and £1,000. This highlights the crisp distinction between the two domains; for the class of subexponential distributions, ruin is more likely to come from a single extreme event than from a series of bad episodes. This logic underpins classical risk theory as outlined by Lundberg early in the 20 th Century and formalized by Cramer, but forgotten by economists in recent times. This indicates that insurance can only work in Medocristan; you should never write an uncapped insurance contract if there is a risk of catastrophe. The point is called the catastrophe principle. Cramer showed insurance could not work outside what he called the Cramer condition, which excludes possible ruin from single shocks. With fat tail distributions, extreme events away from the centre of the distribution play a very large role. Black swans are not more frequent, they are more consequential. The fattest tail distribution has just one very large extreme deviation, rather than many departures form the norm. There are three types of fat tails based on mathematical properties. First there are entry level fat tails. This is any distribution with fatter tails than the Gaussian i.e. with more observations within one sigma and with kurtosis (a function of the fourth central moment) higher than three. Second, there are subexponential distributions. LogNormal: The subexponential class includes the lognormal, which is one of the strangest things on earth because sometimes it cheats and moves up to the top of the diagram. At low variance, it is thin-tailed, at high variance, it behaves like the very fat tailed. in-tailed, at high variance, it behaves like the very fat tailed. Membership in the subexponential class satisfies the Cramer condition of possibility of insurance (losses are more likely to come from many events than a single one) Technically it means that the expectation of the exponential of the random variable exists. Third, what is called by a variety of names, the power law, or slowly varying class, or “Pareto tails” class correspond to real fat tails. The traditional statisticians approach to fat tails has been to assume a different distribution but keep doing business as usual, using same metrics, tests, and statements of significance. But this is not how it really works and they fall into logical inconsistencies. Once we are outsaide the zone for which statistical techniques were designed, things no longer work as planned. Here are some consequences The law of large numbers, when it works, works too slowly in the real world (this is more shocking than you think as it cancels most statistical estimators) The mean of the distribution will not correspond to the sample mean. In fact, there is no fat tailed distribution in which the mean can be properly estimated directly from the sample mean, unless we have orders of magnitude more data than we do Standard deviations and variance are not useable. They fail out of sample. Beta, Sharpe Ratio and other common financial metrics are uninformative. Robust statistics is not robust at all. The so-called “empirical distribution” is not empirical (as it misrepresents the expected payoffs in the tails). Linear regression doesn’t work. Maximum likelihood methods work for parameters (good news). We can have plug in estimators in some situations. The gap between dis-confirmatory and confirmatory empiricism is wider than in situations covered by common statistics i.e. difference between absence of evidence and evidence of absence becomes larger. Principal component analysis is likely to produce false factors. Methods of moments fail to work. Higher moments are uninformative or do not exist. There is no such thing as a typical large deviation: conditional on having a large move, such move is not defined. The Gini coefficient ceases to be additive. It becomes super-additive. The Gini gives an illusion of large con- centrations of wealth. (In other words, inequality in a continent, say Europe, can be higher than the average inequality of its members). While it takes 30 observations in the Gaussian to stabilize the mean up to a given level, it takes \\(10^{11}\\) observations in the Pareto to bring the sample error down by the same amount (assuming the mean exists). You cannot make claims about the stability of the sample mean with a fat tailed distribution. There are other ways to do this, but not from observations on the sample mean. We have known at least since Sextus Empiricus that we cannot rule out degeneracy but there are situations in which we can rule out non-degeneracy. If I see a distribution that has no randomness, I cannot say it is not random. That is, we cannot say there are no black swans. Let us now add one observation. I can now see it is random, and I can rule out degeneracy. I can say it is not not random. On the right hand side we have seen a black swan, therefore the statement that there are no black swans is wrong. This is the negative empiricism that underpins Western science. As we gather information, we can rule things out. If we see a 20 sigma event, we can rule out that the distribution is thin-tailed. Pareto - Scalability The intuition behind the Pareto Law. It is simply defined as: say X is a random variable. For x sufficently large, the probability of exceeding 2x divided by the probability of exceeding x is no different from the probability of exceeding 4x divided by the probability of exceeding 2x, and so forth. So if we have a Pareto (or Pareto-style) distribution, the ratio of people with £16 million compared to £8 million is the same as the ratio of people with £2 million and £1 million. There is a constant inequality. This distribution has no characteristic scale which makes it very easy to understand. Although this distribution often has no mean and no standard deviation we still understand it. But because it has no mean we have to ditch the statistical textbooks and do something more solid, more rigorous. A Pareto distribution has no higher moments: moments either do not exist or become statistically more and more unstable. In 2009 I took 55 years of data and looked at how much of the kurtosis (a function of the fourth moment) came from the largest observation. For a Gaussian the maximum contribution over the same time span should be around .008 ± .0028. For the S&amp;P 500 it was about 80 per cent. This tells us that we dont know anything about kurtosis. Its sample error is huge; or it may not exist so the measurement is heavily sample dependent. If we dont know anything about the fourth moment, we know nothing about the stability of the second moment. It means we are not in a class of distribution that allows us to work with the variance, even if it exists. This is finance. We cannot use standard statistical methods with financial data. Financial data, debunks all the college textbooks we are currently using Econometrics that deals with squares goes out of the window. The variance of the squares is analogous to the fourth moment. The variance of the squares is analogous to the fourth moment. We do not know the variance. But we can work very easily with Pareto distributions. They give us less information, but nevertheless, it is more rigorous if the data are uncapped or if there are any open variables. Principal component analysis is a dimension reduction method for big data and it works beautifully with thin tails. But if there is not enough data there is an illusion of a structure. As we increase the data (the n variables), the structure becomes flat. Lessons: Once we know something is fat-tailed, we can use heuristics to see how an exposure there reacts to random events: how much is a given unit harmed by them. It is vastly more effective to focus on being insulated from the harm of random events than try to figure them out in the required details (as we saw the inferential errors under fat tails are huge). So it is more solid, much wiser, more ethical, and more effective to focus on detection heuristics and policies rather than fabricate statistical properties. The beautiful thing we discovered is that everything that is fragile has to present a concave exposure similar –if not identical –to the payoff of a short option, that is, a negative exposure to volatility. It is nonlinear, necessarily. It has to have harm that accelerates with intensity, up to the point of breaking. If I jump 10m I am harmed more than 10 times than if I jump one metre. That is a necessary property of fragility. We just need to look at acceleration in the tails. We have built effective stress testing heuristics based on such an option-like property. In the real world we want simple things that work; we want to impress our accountant and not our peers. (My argument in the latest instalment of the Incerto, Skin in the Game is that systems judged by peers and not evolution rot from overcomplication). To survive we need to have clear techniques that map to our procedural intuitions. The new focus is on how to detect and measure convexity and concavity. This is much, much simpler than probability. Taleb (2017) Darwin Colleges(pdf) 2.2 Statistical Consequences of Fat Tails Conventional statistics fail to cover fat tails; physicists who use power laws do not usually produce statistical estimators. Taleb’s Research Site Take nothing for granted - It is what it is. Another 300 years of data is required to test a statistical hypothesis. A dataset has no variance. A distribution’s standard deviation will not converge in a lifetime’s worth of data. Fat tailed random variables challenge our conceptions of mean and standard deviation. Linear regression also breaks under fat tails. The convincing case is made that power law distributions should be the default for modeling data rather than the thin-tailed Normal distribution. Any distribution with more density in the tails than the Normal distribution is said to have thick tails. This corresponds to raw kurtosis &gt; 3. The tail density needs to decay slower than Normal, \\(\\frac{-x^2}{e^{2\\sigma^2}}\\). Fat tailed distributions are the thickest tailed distributions. The power law is an example of this - they’re the distributions with so much additional density in their tails that moments \\(E[X^p]\\) are no longer finite. 2.2.1 Power Law Distributions 2.2.1.1 Pareto Distribution Pareto discovered that 20% percent of taxpayers had 80% of the income across countries in Europe. One parameter of the Pareto power law distribution is α, which is known as the tail index. Pareto’s 80-20 example corresponds to α = 1.16. The tail index describes the behavior of density decay in the tail, as its name implies. The strange thing about power law distributions is that, depending on the tail index α, some of its moments may not exist or be infinite. There is no finite mean if α &lt; 1, and there is no finite variance if α &lt; 2. The same applies for skewness at α &lt; 3 and kurtosis when α &lt; 4, and so on. The tails get thicker as α gets smaller. Pseudo-convergence: A tail index less than 2 doesn’t mean that we can’t compute the sample variance of dataset. Rather, betting on the stability of the variance is unwise because this sample variance will never converge, and can in fact “spike” at any time. Furthermore, if the 4th moment (kurtosis) doesn’t exist, this may imply unbearably slow convergence of the 2nd moment (variance). The Central Limit Theorem, which is typically very useful for sums and averages, requires a finite variance, so tail indices α &lt; 2 do not obey. The assumption for the analytic Black-Scholes-Merton price for a financial option - that the random walk sum of movements converges to the Normal distribution - is also violated, so that breaks too. If the tail index is slightly over 2, it will converge to the Normal in the limit, but very slowly. Tail events - the unlikely events of the atypically large magnitude - are the most indicative of the tail behavior. But these tail events are rare. Without a deep understanding of the underlying process which has generated these samples, it can be tough to rule out that the data was generated by a power law. In this sense, we might consider that “most” processes are fat tailed by default - or, we should at least assume they are until we have enough quantitative or qualitative data to prove otherwise. Review of Taleb (Gelman) 2.3 Lindy Effect Increasing lifetime expections If a book has been in print for forty years, I can expect it to be in print for another forty years. But, and that is the main difference, if it survives another decade, then it will be expected to be in print another fifty years. This, simply, as a rule, tells you why things that have been around for a long time are not “aging” like persons, but “aging” in reverse. Every year that passes without extinction doubles the additional life expectancy. This is an indicator of some robustness. The robustness of an item is proportional to its life! (Taleb) Pareto Lindy Lifetimes following the Pareto distribution (a power-law distribution) demonstrate the Lindy effect. With the parameter \\(α = 2\\), conditional on reaching an age of \\(x &gt; x_{min}\\), the expected future lifetime is also \\(x\\). Initially the expected lifetime is \\(2 x_{min}\\) but if that point is reached then the expected future lifetime is also \\(2 x_{min}\\); if that point is reached making the total lifetime so far \\(4 x_{min}\\) then the expected future lifetime is \\(4 x_{min}\\); and so on. More generally with proportionality rather than equality, given \\(m &gt; 0\\) and using the parameter \\(\\alpha = \\frac {m}{m-1}\\) in the Pareto distribution, conditional on reaching any age of \\(x &gt; x_{min}\\), the expected future lifetime is \\(( m − 1 ) x\\). Example: for \\(α = 2\\) or \\(m = 2\\) the expected future lifetime is \\(x\\) . The Lindy effect is connected to Pareto’s Law, to Zipf’s Law, and to socioeconomic inequality. Wikipedia: Lindy Effect Current age is the only piece of information we know at time 0. In the absence of all information except for current age, the best estimator for future life expectancy is the current age. In real life, however, there is usually far more information you can incorporate. The Lindy Effect is thus a trivial heuristics, the usefulness of which decreases as if you acquire additional relevant information. Wang: Lindy Fallacy Doan: Lindy Simulation Levchuk: Testing Lindy "],["hypothesis-testing.html", "3 Hypothesis Testing 3.1 Connecting to Theory 3.2 GLMM 3.3 Logit 3.4 Probit", " 3 Hypothesis Testing 3.1 Connecting to Theory Memo In order to bound the probability of Type 2 errors below a small value we may have to accept a high probability of making a Type 1 error. Scheel Abstract For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound “derivation chain” between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology’s reform movement and help us to develop strong, testable theories, as Paul Meehl urged. Memo Scheel Excessive leniency in study design, data collection, and analysis led psy- chological scientists to be overconfident about many hypotheses that turned out to be false. In response, psy- chological science as a field tightened the screws on the machinery of confirmatory testing: Predictions should be more specific, designs more powerful, and statistical tests more stringent, leaving less room for error and misrepre- sentation. Confirmatory testing will be taught as a highly formalized protocol with clear rules, and the student will learn to strictly separate it from the “exploratory” part of the research process. Has learned how to operate the hypothesis-testing machinery but not how to feed it with meaningful input. When setting up a hypothesis test, the researcher has to specify how their independent and dependent variables will be operationalized, how many participants they will collect, which exclusion criteria they will apply, which statistical method they will use, how to decide whether the hypothesis was corroborated or falsified, and so on. But deciding between these myriad options often feels like guesswork. A lack of knowledge about the elements that link their test back to the theory from which their hypothesis was derived. By using arbitrary defaults and heuristics to bridge these gaps, the researcher cannot be sure how their test result informs the theory. Scheel(2020) Less Hypothesis Testing (pdf) 3.2 GLMM The advent of generalized linear models has allowed us to build regression-type models of data when the distribution of the response variable is non-normal–for example, when your DV is binary. (If you would like to know a little more about GLiMs, I wrote a fairly extensive answer here, which may be useful although the context differs.) However, a GLiM, e.g. a logistic regression model, assumes that your data are independent. For instance, imagine a study that looks at whether a child has developed asthma. Each child contributes one data point to the study–they either have asthma or they don’t. Sometimes data are not independent, though. Consider another study that looks at whether a child has a cold at various points during the school year. In this case, each child contributes many data points. At one time a child might have a cold, later they might not, and still later they might have another cold. These data are not independent because they came from the same child. In order to appropriately analyze these data, we need to somehow take this non-independence into account. There are two ways: One way is to use the generalized estimating equations (which you don’t mention, so we’ll skip). The other way is to use a generalized linear mixed model. GLiMMs can account for the non-independence by adding random effects (as (MichaelChernick?) notes). Thus, the answer is that your second option is for non-normal repeated measures (or otherwise non-independent) data. (I should mention, in keeping with (Macro?)’s comment, that general-ized linear mixed models include linear models as a special case and thus can be used with normally distributed data. However, in typical usage the term connotes non-normal data.) Update: (The OP has asked about GEE as well, so I will write a little about how all three relate to each other.) Here’s a basic overview: a typical GLiM (I’ll use logistic regression as the prototypical case) lets you model an independent binary response as a function of covariates a GLMM lets you model a non-independent (or clustered) binary response conditional on the attributes of each individual cluster as a function of covariates the GEE lets you model the population mean response of non-independent binary data as a function of covariates Since you have multiple trials per participant, your data are not independent; as you correctly note, “[t]rials within one participant are likely to be more similar than as compared to the whole group.” Therefore, you should use either a GLMM or the GEE. The issue, then, is how to choose whether GLMM or GEE would be more appropriate for your situation. The answer to this question depends on the subject of your research–specifically, the target of the inferences you hope to make. As I stated above, with a GLMM, the betas are telling you about the effect of a one unit change in your covariates on a particular participant, given their individual characteristics. On the other hand with the GEE, the betas are telling you about the effect of a one unit change in your covariates on the average of the responses of the entire population in question. This is a difficult distinction to grasp, especially because there is no such distinction with linear models (in which case the two are the same thing). One way to try to wrap your head around this is to imagine averaging over your population on both sides of the equals sign in your model. For example, this might be a model: \\[logit(p_i)=β_0+β_1X_1+b_i\\] where: \\(logit(p)=ln(\\frac{p}{1−p})\\), &amp; \\(b∼N(0,σ_{b}^2)\\) There is a parameter that governs the response distribution (pp, the probability, with binary data) on the left side for each participant. On the right hand side, there are coefficients for the effect of the covariate[s] and the baseline level when the covariate[s] equals 0. The first thing to notice is that the actual intercept for any specific individual is not β0_0, but rather (β0+bi)(_0+b_i). But so what? If we are assuming that the bib_i’s (the random effect) are normally distributed with a mean of 0 (as we’ve done), certainly we can average over these without difficulty (it would just be β0_0). Moreover, in this case we don’t have a corresponding random effect for the slopes and thus their average is just β1_1. So the average of the intercepts plus the average of the slopes must be equal to the logit transformation of the average of the pip_i’s on the left, mustn’t it? Unfortunately, no. The problem is that in between those two is the logit, which is a non-linear transformation. (If the transformation were linear, they would be equivalent, which is why this problem doesn’t occur for linear models.) The following plot makes this clear: Imagine that this plot represents the underlying data generating process for the probability that a small class of students will be able to pass a test on some subject with a given number of hours of instruction on that topic. Each of the grey curves represents the probability of passing the test with varying amounts of instruction for one of the students. The bold curve is the average over the whole class. In this case, the effect of an additional hour of teaching conditional on the student’s attributes is β1_1–the same for each student (that is, there is not a random slope). Note, though, that the students baseline ability differs amongst them–probably due to differences in things like IQ (that is, there is a random intercept). The average probability for the class as a whole, however, follows a different profile than the students. The strikingly counter-intuitive result is this: an additional hour of instruction can have a sizable effect on the probability of each student passing the test, but have relatively little effect on the probable total proportion of students who pass. This is because some students might already have had a large chance of passing while others might still have little chance. The question of whether you should use a GLMM or the GEE is the question of which of these functions you want to estimate. If you wanted to know about the probability of a given student passing (if, say, you were the student, or the student’s parent), you want to use a GLMM. On the other hand, if you want to know about the effect on the population (if, for example, you were the teacher, or the principal), you would want to use the GEE. StackOverFlow What are the best methods for checking a generalized linear mixed model (GLMM) for proper fit? Unfortunately, it isn’t as straightforward as it is for a general linear model. n linear models the requirements are easy to outline: linear in the parameters, normally distributed and independent residuals, and homogeneity of variance (that is, similar variance at all values of all predictors). For linear models, there are well-described and well-implemented methods for checking each of these, both visual/descriptive methods and statistical tests. It is not nearly as easy for GLMMs. Assumption: Random effects come from a normal distribution Let’s start with one of the more familiar elements of GLMMs, which is related to the random effects. There is an assumption that random effects—both intercepts and slopes—are normally distributed. These are relatively easy to export to a data set in most statistical software (including SAS and R). Personally, I much prefer visual methods of checking for normal distributions, and typically go right to making histograms or normal probability plots (Q-Q plots) of each of the random effects. If the histograms look roughly bell-shaped and symmetric, or the Q-Q plots generally fall close to a diagonal line, I usually consider this to be good enough. If the random effects are not reasonably normally distributed, however, there are not simple remedies. In a general linear model outcomes can be transformed. In GLMMs they cannot. Research is currently being conducted on the consequences of mis-specifying the distribution of random effects in GLMMs. (Outliers, of course, can be handled the same way as in generalized linear models—except that an entire random subject, as opposed to a single observation, may be examined.) Assumption: The chosen link function is appropriate Additional assumptions of GLMMs are more related to the generalized linear model side. One of these is the relationship of the numeric predictors to the parameter of interest, which is determined by the link function. For both generalized linear models and GLMMs, it is important to understand that the most typical link functions (e.g., the logit for binomial data, the log for Poisson data) are not guaranteed to be a good representation of the relationship of the predictors with the outcomes. Checking this assumption can become quite complicated as models become more crowded with fixed and random effects. One relatively simple (though not perfect) way to approach this is to compare the predicted values to the actual outcomes. With most GLMMs, it is best to compare averages of outcomes to predicted values. For example, with binomial models, one could take all of the values with predicted values near 0.5, 0.15, 0.25, etc., and average the actual outcomes (the 0s and 1s). You can then plot these average values against the predicted values. If the general form of the model is correct, the differences between the predicted values and the averaged actual values will be small. (Of course how small depends on the number of observations and variance function). No “patterns” in these differences should be obvious. This is similar to the idea of the Hosmer-Lemeshow test for logistic regression models. If you suspect that the form of the link function is not correct, there are remedies. Possibilites include changing the link function, transforming numeric predictors, or (if necessary) categorizing continuous predictors. Assumption: Appropriate estimation of variance Finally, it is important to check the variability of the outcomes. This is also not as easy as it is for linear models, since the variance is not constant and is a function of the parameter being estimated. Fortunately, this is one of the easier assumptions to check. One of the fit statistics your statistical software produces is a generalized chi-square that compares the magnitude of the model residuals to the theoretical variance. The chi-square divided by its degrees of freedom should be approximately 1. If this statistic is too large, then the variance is “overdispersed” (larger than it should be). Alternatively, if the statistic is too small, the variance is “underdispersed.” While the best way to approach this varies by distribution, there are options to adjust models for overdispersion that result in more conservative p-values. TheAnalysisFactor 3.3 Logit Possible Analysis methods: Below is a list of some analysis methods you may have encountered. Some of the methods listed are quite reasonable while others have either fallen out of favor or have limitations. Logistic regression, the focus of this page Probit regression. Probit analysis will produce results similar logistic regression. The choice of probit versus logit depends largely on individual preferences. OLS regression. When used with a binary response variable, this model is known as a linear probability model and can be used as a way to describe conditional probabilities. However, the errors (i.e., residuals) from the linear probability model violate the homoskedasticity and normality of errors assumptions of OLS regression, resulting in invalid standard errors and hypothesis tests. For a more thorough discussion of these and other problems with the linear probability model. Two-group discriminant function analysis. A multivariate method for dichotomous outcome variables. Hotelling’s T2. The 0/1 outcome is turned into the grouping variable, and the former predictors are turned into outcome variables. This will produce an overall test of significance but will not give individual coefficients for each variable, and it is unclear the extent to which each “predictor” is adjusted for the impact of the other “predictors.” ucla If you want to interpret the estimated effects as relative odds ratios, just do exp(coef(x)) (gives you \\(e^β\\), the multiplicative change in the odds ratio for \\(y=1\\) if the covariate associated with \\(β\\) increases by 1). For profile likelihood intervals for this quantity, you can do require(MASS) exp(cbind(coef(x), confint(x))) StackOverFlow 3.4 Probit A standard linear model (e.g., a simple regression model) can be thought of as having two ‘parts.’ These are called the structural component and the random component. For example: \\[Y=β_0+β_1 X+ε\\] where \\(ε∼N(0,σ^2)\\) The first two terms (that is, \\(β_0 + β_1 X\\)) constitute the structural component, and the \\(ε\\) (which indicates a normally distributed error term) is the random component. When the response variable is not normally distributed (for example, if your response variable is binary) this approach may no longer be valid. The generalized linear model (GLiM) was developed to address such cases, and logit and probit models are special cases of GLiMs that are appropriate for binary variables (or multi-category response variables with some adaptations to the process). A GLiM has three parts, a structural component, a link function, and a response distribution. For example: \\[g(μ)=β_0+β_1 X\\] Here \\(β_0 + β_1 X\\) is again the structural component, \\(g()\\) is the link function, and \\(μ\\) is a mean of a conditional response distribution at a given point in the covariate space. The way we think about the structural component here doesn’t really differ from how we think about it with standard linear models; in fact, that’s one of the great advantages of GLiMs. Because for many distributions the variance is a function of the mean, having fit a conditional mean (and given that you stipulated a response distribution), you have automatically accounted for the analog of the random component in a linear model (N.B.: this can be more complicated in practice). The link function is the key to GLiMs: since the distribution of the response variable is non-normal, it’s what lets us connect the structural component to the response– it ‘links’ them (hence the name). It’s also the key to your question, since the logit and probit are links, and understanding link functions will allow us to intelligently choose when to use which one. Although there can be many link functions that can be acceptable, often there is one that is special. Without wanting to get too far into the weeds (this can get very technical) the predicted mean, \\(μ\\), will not necessarily be mathematically the same as the response distribution’s canonical location parameter; the link function that does equate them is the canonical link function. The advantage of this \"is that a minimal sufficient statistic for \\(β\\). The canonical link for binary response data (more specifically, the binomial distribution) is the logit. However, there are lots of functions that can map the structural component onto the interval (0,1)(0,1), and thus be acceptable; the probit is also popular, but there are yet other options that are sometimes used (such as the complementary log log, \\(ln(−ln(1−μ))\\), often called cloglog). Thus, there are lots of possible link functions and the choice of link function can be very important. The choice should be made based on some combination of: Knowledge of the response distribution, Theoretical considerations, and Empirical fit to the data. These considerations can be used to guide your choice of link. To start with, if your response variable is the outcome of a Bernoulli trial (that is, 0 or 1), your response distribution will be binomial, and what you are actually modeling is the probability of an observation being a 1 (that is, \\(π(Y=1)\\). As a result, any function that maps the real number line, \\((−∞,+∞)\\) to the interval \\((0,1)\\) will work. If you are thinking of your covariates as directly connected to the probability of success, then you would typically choose logistic regression because it is the canonical link. However, consider the following example: You are asked to model high_Blood_Pressure as a function of some covariates. Blood pressure itself is normally distributed in the population. Clinicians dichotomized it during the study (that is, they only recorded ‘high-BP’ or ‘normal’). In this case, probit would be preferable a-priori for theoretical reasons. Your binary outcome depends on a hidden Gaussian variable. Another consideration is that both logit and probit are symmetrical, if you believe that the probability of success rises slowly from zero, but then tapers off more quickly as it approaches one, the cloglog is called for. Lastly, note that the empirical fit of the model to the data is unlikely to be of assistance in selecting a link, unless the shapes of the link functions in question differ substantially (of which, the logit and probit do not). For instance, consider the following simulation: set.seed(1) probLower = vector(length=1000) for(i in 1:1000){ x = rnorm(1000) y = rbinom(n=1000, size=1, prob=pnorm(x)) logitModel = glm(y~x, family=binomial(link=&quot;logit&quot;)) probitModel = glm(y~x, family=binomial(link=&quot;probit&quot;)) probLower[i] = deviance(probitModel)&lt;deviance(logitModel) } sum(probLower)/1000 [1] 0.695 Even when we know the data were generated by a probit model, and we have 1000 data points, the probit model only yields a better fit 70% of the time, and even then, often by only a trivial amount. Consider the last iteration: deviance(probitModel) [1] 1025.759 deviance(logitModel) [1] 1026.366 deviance(logitModel)-deviance(probitModel) [1] 0.6076806 The reason for this is simply that the logit and probit link functions yield very similar outputs when given the same inputs. The logit and probit functions are practically identical, except that the logit is slightly further from the bounds when they ‘turn the corner.’ (Note that to get the logit and the probit to align optimally, the logit’s \\(β_1\\) must be \\(≈1.7\\) times the corresponding slope value for the probit. In addition, I could have shifted the cloglog over slightly so that they would lay on top of each other more, but I left it to the side to keep the figure more readable. Notice that the cloglog is asymmetrical whereas the others are not; it starts pulling away from 0 earlier, but more slowly, and approaches close to 1 and then turns sharply. A couple more things can be said about link functions. First, considering the identity function \\((g(η)=ηg(\\eta)=\\eta)\\) as a link function allows us to understand the standard linear model as a special case of the generalized linear model (that is, the response distribution is normal, and the link is the identity function). It’s also important to recognize that whatever transformation the link instantiates is properly applied to the parameter governing the response distribution (that is, \\(μ\\)), not the actual response data. Finally, because in practice we never have the underlying parameter to transform, in discussions of these models, often what is considered to be the actual link is left implicit and the model is represented by the inverse of the link function applied to the structural component instead. That is: \\[μ=g^{−1}(β_0+β_1 X)\\] For instance, logistic regression is usually represented: \\[π(Y)=\\frac{exp(β_0 + β_1 X)}{1+exp(β_0 + β_1 X)}\\] instead of: \\[ln(\\frac{(π(Y)}{1−π(Y)}) = β_0 + β_1 X\\] For a quick and clear, but solid, overview of the generalized linear model, see chapter 10 of Fitzmaurice, Laird, &amp; Ware (2004), For how to fit these models in R, check out the documentation for the function ?glm in the base package. (One final note added later:) I occasionally hear people say that you shouldn’t use the probit, because it can’t be interpreted. This is not true, although the interpretation of the betas is less intuitive. With logistic regression, a one unit change in \\(X_1\\) is associated with a \\(β_1\\) change in the log odds of ‘success’ (alternatively, an \\(exp(β_1)\\)-fold change in the odds), all else being equal. With a probit, this would be a change of \\(β_1 z\\)’s. (Think of two observations in a dataset with \\(z\\)-scores of 1 and 2, for example.) To convert these into predicted probabilities, you can pass them through the normal CDF, or look them up on a \\(z\\)-table. StackOverFlow "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Some Other Dyrehaugen sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistsics (loc) rurb - On Urbanization (loc) rvar - On Various Other Issues (loc) rwsd - On Wisdom (loc) jde - Blog in English (loc) jdn - Blog in Norwegian (loc) jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS", " C NEWS "]]
